{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08b36226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data_With_features.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data_with_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "50b8d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_students = pd.read_csv(\"processed_train_student.csv\")\n",
    "# Rename uid to id to match train_data\n",
    "train_students = train_students.rename(columns={\"UID\": \"id\"})\n",
    "# Merge train data with student data\n",
    "merged_df = pd.merge(train_data, train_students, on=\"id\", how=\"left\")\n",
    "train_data = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "80a2834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # adjust max_features if needed\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(train_data[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f6dcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_features = ['char_count', 'word_count', 'java_keyword_count', \n",
    "                 'method_count', 'class_count', \n",
    "                 'NaN_count', 'comment_count']\n",
    "\n",
    "X_struct = train_data[hand_features].fillna(0).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "67768190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ScoreScaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee2b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_tfidf = hstack([tfidf_embeddings, X_struct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23aae7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:04<00:00, 10.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load pretrained Sentence-BERT\n",
    "bert_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "def get_bert_embedding(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state  # (1, seq_len, hidden_size)\n",
    "            # Average pooling\n",
    "            embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "bert_embeddings = get_bert_embedding(train_data[\"answers\"].tolist())\n",
    "\n",
    "bert_array = np.array(bert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct  # already dense\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Now safely combine them\n",
    "X_combined_bert = np.hstack([bert_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "682618fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:10<00:00,  4.13it/s]\n"
     ]
    }
   ],
   "source": [
    "codebert_model_name = \"microsoft/codebert-base\"\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(codebert_model_name)\n",
    "codebert_model = AutoModel.from_pretrained(codebert_model_name)\n",
    "\n",
    "def get_codebert_embedding(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = codebert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = codebert_model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "codebert_embeddings = get_codebert_embedding(train_data[\"answers\"].tolist())\n",
    "\n",
    "codebert_array = np.array(codebert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "X_combined_codebert = np.hstack([codebert_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "10292350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_texts = train_data[\"answers\"].apply(simple_preprocess).tolist()\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "def average_vector(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "word2vec_array = np.array([average_vector(tokens, w2v_model, 100) for tokens in tokenized_texts])\n",
    "    \n",
    "X_struct_dense = X_struct.toarray() if hasattr(X_struct, \"toarray\") else X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Combine\n",
    "X_combined_w2v = np.hstack([word2vec_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acf9762d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 10.01it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for test data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # adjust max_features if needed\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(test_data[\"answers\"])\n",
    "\n",
    "hand_features = ['char_count', 'word_count', 'java_keyword_count', \n",
    "                 'method_count', 'class_count', \n",
    "                 'NaN_count', 'comment_count']\n",
    "\n",
    "X_struct = test_data[hand_features].fillna(0).values \n",
    "\n",
    "Test_X_tfidf = hstack([tfidf_embeddings, X_struct])\n",
    "\n",
    "#---------\n",
    "\n",
    "bert_embeddings = get_bert_embedding(test_data[\"answers\"].tolist())\n",
    "\n",
    "bert_array = np.array(bert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct  # already dense\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Now safely combine them\n",
    "Test_X_combined_bert = np.hstack([bert_array, X_struct_scaled])\n",
    "\n",
    "#---------\n",
    "\n",
    "codebert_embeddings = get_codebert_embedding(test_data[\"answers\"].tolist())\n",
    "\n",
    "codebert_array = np.array(codebert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "Test_X_combined_codebert = np.hstack([codebert_array, X_struct_scaled])\n",
    "\n",
    "#---------\n",
    "tokenized_texts = test_data[\"answers\"].apply(simple_preprocess).tolist()\n",
    "\n",
    "word2vec_array = np.array([average_vector(tokens, w2v_model, 100) for tokens in tokenized_texts])\n",
    "    \n",
    "X_struct_dense = X_struct.toarray() if hasattr(X_struct, \"toarray\") else X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Combine\n",
    "Test_X_combined_w2v = np.hstack([word2vec_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "760a7b2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.18351869, 41.01678813,  7.56765573, 13.01352715,  6.47608599,\n",
       "       11.85045307, 17.59043877, 16.97506138, 23.07993855, 10.77043512,\n",
       "       12.18307473,  8.02037458, 11.46845305])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "X_train = X_combined_w2v\n",
    "y_train = train_data[\"FinalClass\"].values  # Assuming 'FinalClass' is the target variable\n",
    "X_test = Test_X_combined_w2v\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbd1481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec -> MSE: 54.2164, R²: -2.7663\n",
      "TF-IDF -> MSE: 25.9406, R²: -0.8020\n",
      "BERT -> MSE: 212.6893, R²: -13.7752\n",
      "CodeBERT -> MSE: 191.9950, R²: -12.3376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "embeddings = {\n",
    "    \"Word2Vec\" : X_combined_w2v,\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BERT\": X_combined_bert,\n",
    "    \"CodeBERT\": X_combined_codebert,\n",
    "}\n",
    "\n",
    "y = train_data[\"FinalClass\"].values  # Assuming 'score' is the target variable\n",
    "#y = ScoreScaler.fit_transform(y.reshape(-1, 1)).flatten()  # Scale the target variable\n",
    "\n",
    "for name, X in embeddings.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} -> MSE: {mse:.4f}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67db3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: BERT ===\n",
      "\n",
      "=== Embedding: CodeBERT ===\n",
      "\n",
      "=== Embedding: Word2Vec ===\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "\"\"\"# Embedding dictionaries: add your actual numpy arrays here\n",
    "embeddings = {\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BERT\": X_combined_bert,\n",
    "    \"CodeBERT\": X_combined_codebert,\n",
    "    \"Word2Vec\": X_combined_w2v\n",
    "}\"\"\"\n",
    "\n",
    "# Regression models to try\n",
    "regressors = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"MLP\": MLPRegressor(max_iter=10000)\n",
    "}\n",
    "\n",
    "# Target values\n",
    "y = train_data[\"FinalClass\"].values\n",
    "\n",
    "word2vec_array = scaler.fit_transform(X_combined_w2v)\n",
    "bert_array = scaler.fit_transform(X_combined_bert)\n",
    "codebert_array = scaler.fit_transform(X_combined_codebert)\n",
    "\n",
    "embeddings = {\n",
    "    \"BERT\": bert_array,\n",
    "    \"CodeBERT\": codebert_array,\n",
    "    \"Word2Vec\": word2vec_array\n",
    "}\n",
    "\n",
    "\n",
    "# Run all combos\n",
    "for embed_name, X in embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    #X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "\n",
    "    if embed_name == \"TF-IDF\":\n",
    "        X_test = Test_X_tfidf\n",
    "    elif embed_name == \"BERT\":\n",
    "        X_test = Test_X_combined_bert\n",
    "    elif embed_name == \"CodeBERT\":\n",
    "        X_test = Test_X_combined_codebert\n",
    "    elif embed_name == \"Word2Vec\":\n",
    "        X_test = Test_X_combined_w2v\n",
    "\n",
    "    \n",
    "    for model_name, model in regressors.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        #y_pred = ScoreScaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "\n",
    "        # Combine with test data write to CSV\n",
    "        test_data[\"FinalClass\"] = y_pred\n",
    "        test_data = test_data[[\"id\",\"FinalClass\"]]\n",
    "        test_data.to_csv(f\"predictions/{embed_name}_{model_name}_predictions_scaled.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46267087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: BERT ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\model_selection\\_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(param_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'mlp__learning_rate': 'constant', 'mlp__max_iter': 1000, 'mlp__solver': 'adam'}\n",
      "Best score (negative MSE): -50.19595597593614\n",
      "Validation MSE: 30.38662233154869\n",
      "\n",
      "=== Embedding: CodeBERT ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\model_selection\\_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(param_list)\n",
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'mlp__activation': 'tanh', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100,), 'mlp__learning_rate': 'constant', 'mlp__max_iter': 1000, 'mlp__solver': 'adam'}\n",
      "Best score (negative MSE): -37.376400625931446\n",
      "Validation MSE: 10.122734490111133\n",
      "\n",
      "=== Embedding: Word2Vec ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best parameters: {'mlp__activation': 'relu', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (64, 64, 32), 'mlp__learning_rate': 'constant', 'mlp__max_iter': 1000, 'mlp__solver': 'adam'}\n",
      "Best score (negative MSE): -22.60937962132116\n",
      "Validation MSE: 46.438678025654795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\model_selection\\_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(param_list)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "embeddings = {\n",
    "    \"BERT\": X_combined_bert,\n",
    "    \"CodeBERT\": X_combined_codebert,\n",
    "    \"Word2Vec\": X_combined_w2v\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('mlp', mlp)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(100,), (50, 50), (100, 50), (64, 64, 32)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__solver': ['adam'],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01],  # L2 penalty\n",
    "    'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "    'mlp__max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "for embed_name, X in embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "    print(\"Best score (negative MSE):\", grid.best_score_)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(\"Validation MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dba0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: BERT ===\n",
      "LinearRegression     | MSE: 248.131\n",
      "Ridge                | MSE: 28.938\n",
      "Lasso                | MSE: 9.821\n",
      "ElasticNet           | MSE: 9.109\n",
      "DecisionTree         | MSE: 22.222\n",
      "RandomForest         | MSE: 6.364\n",
      "GradientBoosting     | MSE: 7.903\n",
      "SVR                  | MSE: 18.959\n",
      "KNN                  | MSE: 11.938\n",
      "MLP                  | MSE: 27.103\n",
      "\n",
      "=== Embedding: CodeBERT ===\n",
      "LinearRegression     | MSE: 197.692\n",
      "Ridge                | MSE: 18.696\n",
      "Lasso                | MSE: 10.109\n",
      "ElasticNet           | MSE: 7.956\n",
      "DecisionTree         | MSE: 31.111\n",
      "RandomForest         | MSE: 6.288\n",
      "GradientBoosting     | MSE: 8.655\n",
      "SVR                  | MSE: 18.894\n",
      "KNN                  | MSE: 11.818\n",
      "MLP                  | MSE: 8.500\n",
      "\n",
      "=== Embedding: Word2Vec ===\n",
      "LinearRegression     | MSE: 38.895\n",
      "Ridge                | MSE: 18.057\n",
      "Lasso                | MSE: 5.087\n",
      "ElasticNet           | MSE: 5.675\n",
      "DecisionTree         | MSE: 20.889\n",
      "RandomForest         | MSE: 6.913\n",
      "GradientBoosting     | MSE: 8.805\n",
      "SVR                  | MSE: 14.809\n",
      "KNN                  | MSE: 8.147\n",
      "MLP                  | MSE: 61.212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "word2vec_array = scaler.fit_transform(X_combined_w2v)\n",
    "bert_array = scaler.fit_transform(X_combined_bert)\n",
    "codebert_array = scaler.fit_transform(X_combined_codebert)\n",
    "#tfidf_array = scaler.fit_transform(X_tfidf)\n",
    "\n",
    "# Embedding dictionaries: add your actual numpy arrays here\n",
    "embeddings = {\n",
    "    \"BERT\": bert_array,\n",
    "    \"CodeBERT\": codebert_array,\n",
    "    \"Word2Vec\": word2vec_array\n",
    "}\n",
    "\n",
    "# Regression models to try\n",
    "regressors = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"MLP\": MLPRegressor(max_iter=10000)\n",
    "}\n",
    "\n",
    "# Target values\n",
    "y = train_data[\"FinalClass\"].values\n",
    "\n",
    "# Run all combos\n",
    "for embed_name, X in embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for model_name, model in regressors.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "        print(f\"{model_name:<20} | MSE: {mse:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitirme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
