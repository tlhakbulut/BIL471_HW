{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08b36226",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_data = pd.read_csv(\"data/train_data_With_features.csv\")\n",
    "test_data = pd.read_csv(\"data/test_data_with_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50b8d963",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_students = pd.read_csv(\"processed_train_student.csv\")\n",
    "# Rename uid to id to match train_data\n",
    "train_students = train_students.rename(columns={\"UID\": \"id\"})\n",
    "# Merge train data with student data\n",
    "merged_df = pd.merge(train_data, train_students, on=\"id\", how=\"left\")\n",
    "train_data = merged_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c41da61",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a2834b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # adjust max_features if needed\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(train_data[\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f6dcec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hand_features = ['char_count', 'word_count', 'java_keyword_count', \n",
    "                 'method_count', 'class_count', \n",
    "                 'NaN_count', 'comment_count']\n",
    "\n",
    "X_struct = train_data[hand_features].fillna(0).values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cee2b3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "X_tfidf = hstack([tfidf_embeddings, X_struct])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23aae7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 44/44 [00:04<00:00, 10.64it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load pretrained Sentence-BERT\n",
    "bert_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "bert_model = AutoModel.from_pretrained(bert_model_name)\n",
    "\n",
    "def get_bert_embedding(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = bert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = bert_model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state  # (1, seq_len, hidden_size)\n",
    "            # Average pooling\n",
    "            embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "bert_embeddings = get_bert_embedding(train_data[\"answers\"].tolist())\n",
    "\n",
    "bert_array = np.array(bert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct  # already dense\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Now safely combine them\n",
    "X_combined_bert = np.hstack([bert_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "682618fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44/44 [00:10<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "codebert_model_name = \"microsoft/codebert-base\"\n",
    "codebert_tokenizer = AutoTokenizer.from_pretrained(codebert_model_name)\n",
    "codebert_model = AutoModel.from_pretrained(codebert_model_name)\n",
    "\n",
    "def get_codebert_embedding(texts):\n",
    "    embeddings = []\n",
    "    for text in tqdm(texts):\n",
    "        inputs = codebert_tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = codebert_model(**inputs)\n",
    "            last_hidden_state = outputs.last_hidden_state\n",
    "            embedding = last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        embeddings.append(embedding)\n",
    "    return embeddings\n",
    "\n",
    "codebert_embeddings = get_codebert_embedding(train_data[\"answers\"].tolist())\n",
    "\n",
    "codebert_array = np.array(codebert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "X_combined_codebert = np.hstack([codebert_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10292350",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "tokenized_texts = train_data[\"answers\"].apply(simple_preprocess).tolist()\n",
    "w2v_model = Word2Vec(sentences=tokenized_texts, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "def average_vector(tokens, model, vector_size):\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(vector_size)\n",
    "    \n",
    "word2vec_array = np.array([average_vector(tokens, w2v_model, 100) for tokens in tokenized_texts])\n",
    "    \n",
    "X_struct_dense = X_struct.toarray() if hasattr(X_struct, \"toarray\") else X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Combine\n",
    "X_combined_w2v = np.hstack([word2vec_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd1481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec -> MSE: 45.5493, R²: -2.1642\n",
      "TF-IDF -> MSE: 25.9406, R²: -0.8020\n",
      "BERT -> MSE: 212.6893, R²: -13.7752\n",
      "CodeBERT -> MSE: 191.9950, R²: -12.3376\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "embeddings = {\n",
    "    \"Word2Vec\" : X_combined_w2v,\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BERT\": X_combined_bert,\n",
    "    \"CodeBERT\": X_combined_codebert,\n",
    "}\n",
    "\n",
    "y = train_data[\"FinalClass\"].values  # Assuming 'score' is the target variable\n",
    "\n",
    "for name, X in embeddings.items():\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    print(f\"{name} -> MSE: {mse:.4f}, R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67db3214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: TF-IDF ===\n",
      "LinearRegression     | MSE: 25.941\n",
      "Ridge                | MSE: 12.774\n",
      "Lasso                | MSE: 12.926\n",
      "ElasticNet           | MSE: 13.666\n",
      "DecisionTree         | MSE: 28.333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:656: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.4889022974901991, tolerance: 0.09547428571428573\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n",
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\linear_model\\_coordinate_descent.py:656: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.9619620328793417, tolerance: 0.09547428571428573\n",
      "  model = cd_fast.sparse_enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest         | MSE: 11.616\n",
      "GradientBoosting     | MSE: 14.271\n",
      "SVR                  | MSE: 13.326\n",
      "KNN                  | MSE: 9.538\n",
      "MLP                  | MSE: 5670594.852\n",
      "\n",
      "=== Embedding: BERT ===\n",
      "LinearRegression     | MSE: 212.689\n",
      "Ridge                | MSE: 8.149\n",
      "Lasso                | MSE: 9.798\n",
      "ElasticNet           | MSE: 7.285\n",
      "DecisionTree         | MSE: 13.444\n",
      "RandomForest         | MSE: 6.208\n",
      "GradientBoosting     | MSE: 8.148\n",
      "SVR                  | MSE: 14.407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "[WinError 2] Sistem belirtilen dosyayı bulamıyor\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  warnings.warn(\n",
      "  File \"c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 257, in _count_physical_cores\n",
      "    cpu_info = subprocess.run(\n",
      "  File \"c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\subprocess.py\", line 505, in run\n",
      "    with Popen(*popenargs, **kwargs) as process:\n",
      "  File \"c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\subprocess.py\", line 951, in __init__\n",
      "    self._execute_child(args, executable, preexec_fn, close_fds,\n",
      "  File \"c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\subprocess.py\", line 1436, in _execute_child\n",
      "    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN                  | MSE: 9.693\n",
      "MLP                  | MSE: 45.602\n",
      "\n",
      "=== Embedding: CodeBERT ===\n",
      "LinearRegression     | MSE: 191.995\n",
      "Ridge                | MSE: 8.111\n",
      "Lasso                | MSE: 9.798\n",
      "ElasticNet           | MSE: 7.285\n",
      "DecisionTree         | MSE: 31.111\n",
      "RandomForest         | MSE: 6.806\n",
      "GradientBoosting     | MSE: 9.776\n",
      "SVR                  | MSE: 17.015\n",
      "KNN                  | MSE: 9.093\n",
      "MLP                  | MSE: 13.751\n",
      "\n",
      "=== Embedding: Word2Vec ===\n",
      "LinearRegression     | MSE: 45.549\n",
      "Ridge                | MSE: 7.412\n",
      "Lasso                | MSE: 9.798\n",
      "ElasticNet           | MSE: 7.285\n",
      "DecisionTree         | MSE: 29.000\n",
      "RandomForest         | MSE: 6.331\n",
      "GradientBoosting     | MSE: 6.947\n",
      "SVR                  | MSE: 13.742\n",
      "KNN                  | MSE: 9.618\n",
      "MLP                  | MSE: 60.517\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Embedding dictionaries: add your actual numpy arrays here\n",
    "embeddings = {\n",
    "    \"TF-IDF\": X_tfidf,\n",
    "    \"BERT\": X_combined_bert,\n",
    "    \"CodeBERT\": X_combined_codebert,\n",
    "    \"Word2Vec\": X_combined_w2v\n",
    "}\n",
    "\n",
    "# Regression models to try\n",
    "regressors = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"MLP\": MLPRegressor(max_iter=10000)\n",
    "}\n",
    "\n",
    "# Target values\n",
    "y = train_data[\"FinalClass\"].values\n",
    "\n",
    "# Run all combos\n",
    "for embed_name, X in embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for model_name, model in regressors.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "        print(f\"{model_name:<20} | MSE: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46267087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: BERT ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\model_selection\\_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(param_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'mlp__activation': 'tanh', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (50, 50), 'mlp__learning_rate': 'constant', 'mlp__max_iter': 1000, 'mlp__solver': 'adam'}\n",
      "Best score (negative MSE): -50.19595597593614\n",
      "Validation MSE: 30.38662233154869\n",
      "\n",
      "=== Embedding: CodeBERT ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\model_selection\\_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(param_list)\n",
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'mlp__activation': 'tanh', 'mlp__alpha': 0.0001, 'mlp__hidden_layer_sizes': (100,), 'mlp__learning_rate': 'constant', 'mlp__max_iter': 1000, 'mlp__solver': 'adam'}\n",
      "Best score (negative MSE): -37.376400625931446\n",
      "Validation MSE: 10.122734490111133\n",
      "\n",
      "=== Embedding: Word2Vec ===\n",
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n",
      "Best parameters: {'mlp__activation': 'relu', 'mlp__alpha': 0.01, 'mlp__hidden_layer_sizes': (64, 64, 32), 'mlp__learning_rate': 'constant', 'mlp__max_iter': 1000, 'mlp__solver': 'adam'}\n",
      "Best score (negative MSE): -22.60937962132116\n",
      "Validation MSE: 46.438678025654795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\model_selection\\_search.py:409: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  arr = np.array(param_list)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "mlp = MLPRegressor(random_state=42)\n",
    "\n",
    "embeddings = {\n",
    "    \"BERT\": X_combined_bert,\n",
    "    \"CodeBERT\": X_combined_codebert,\n",
    "    \"Word2Vec\": X_combined_w2v\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', scaler),\n",
    "    ('mlp', mlp)\n",
    "])\n",
    "\n",
    "param_grid = {\n",
    "    'mlp__hidden_layer_sizes': [(100,), (50, 50), (100, 50), (64, 64, 32)],\n",
    "    'mlp__activation': ['relu', 'tanh'],\n",
    "    'mlp__solver': ['adam'],\n",
    "    'mlp__alpha': [0.0001, 0.001, 0.01],  # L2 penalty\n",
    "    'mlp__learning_rate': ['constant', 'adaptive'],\n",
    "    'mlp__max_iter': [1000, 2000]\n",
    "}\n",
    "\n",
    "for embed_name, X in embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    grid = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', verbose=2, n_jobs=-1)\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters:\", grid.best_params_)\n",
    "    print(\"Best score (negative MSE):\", grid.best_score_)\n",
    "\n",
    "    best_model = grid.best_estimator_\n",
    "\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    mse = mean_squared_error(y_val, y_pred)\n",
    "    print(\"Validation MSE:\", mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dba0ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: BERT ===\n",
      "LinearRegression     | MSE: 248.131\n",
      "Ridge                | MSE: 28.938\n",
      "Lasso                | MSE: 9.821\n",
      "ElasticNet           | MSE: 9.109\n",
      "DecisionTree         | MSE: 22.222\n",
      "RandomForest         | MSE: 6.364\n",
      "GradientBoosting     | MSE: 7.903\n",
      "SVR                  | MSE: 18.959\n",
      "KNN                  | MSE: 11.938\n",
      "MLP                  | MSE: 27.103\n",
      "\n",
      "=== Embedding: CodeBERT ===\n",
      "LinearRegression     | MSE: 197.692\n",
      "Ridge                | MSE: 18.696\n",
      "Lasso                | MSE: 10.109\n",
      "ElasticNet           | MSE: 7.956\n",
      "DecisionTree         | MSE: 31.111\n",
      "RandomForest         | MSE: 6.288\n",
      "GradientBoosting     | MSE: 8.655\n",
      "SVR                  | MSE: 18.894\n",
      "KNN                  | MSE: 11.818\n",
      "MLP                  | MSE: 8.500\n",
      "\n",
      "=== Embedding: Word2Vec ===\n",
      "LinearRegression     | MSE: 38.895\n",
      "Ridge                | MSE: 18.057\n",
      "Lasso                | MSE: 5.087\n",
      "ElasticNet           | MSE: 5.675\n",
      "DecisionTree         | MSE: 20.889\n",
      "RandomForest         | MSE: 6.913\n",
      "GradientBoosting     | MSE: 8.805\n",
      "SVR                  | MSE: 14.809\n",
      "KNN                  | MSE: 8.147\n",
      "MLP                  | MSE: 61.212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "word2vec_array = scaler.fit_transform(X_combined_w2v)\n",
    "bert_array = scaler.fit_transform(X_combined_bert)\n",
    "codebert_array = scaler.fit_transform(X_combined_codebert)\n",
    "#tfidf_array = scaler.fit_transform(X_tfidf)\n",
    "\n",
    "# Embedding dictionaries: add your actual numpy arrays here\n",
    "embeddings = {\n",
    "    \"BERT\": bert_array,\n",
    "    \"CodeBERT\": codebert_array,\n",
    "    \"Word2Vec\": word2vec_array\n",
    "}\n",
    "\n",
    "# Regression models to try\n",
    "regressors = {\n",
    "    \"LinearRegression\": LinearRegression(),\n",
    "    \"Ridge\": Ridge(),\n",
    "    \"Lasso\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    \"DecisionTree\": DecisionTreeRegressor(),\n",
    "    \"RandomForest\": RandomForestRegressor(),\n",
    "    \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"KNN\": KNeighborsRegressor(),\n",
    "    \"MLP\": MLPRegressor(max_iter=10000)\n",
    "}\n",
    "\n",
    "# Target values\n",
    "y = train_data[\"FinalClass\"].values\n",
    "\n",
    "# Run all combos\n",
    "for embed_name, X in embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    for model_name, model in regressors.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        mse = mean_squared_error(y_val, y_pred)\n",
    "\n",
    "        print(f\"{model_name:<20} | MSE: {mse:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf91454",
   "metadata": {},
   "source": [
    "Now we will get our data ready to test the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9afa5f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>NaN_count</th>\n",
       "      <th>answers</th>\n",
       "      <th>char_count</th>\n",
       "      <th>word_count</th>\n",
       "      <th>java_keyword_count</th>\n",
       "      <th>method_count</th>\n",
       "      <th>class_count</th>\n",
       "      <th>comment_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4380745</td>\n",
       "      <td>1</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>212997</td>\n",
       "      <td>24761</td>\n",
       "      <td>4249</td>\n",
       "      <td>990</td>\n",
       "      <td>157</td>\n",
       "      <td>378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8190737</td>\n",
       "      <td>4</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>103887</td>\n",
       "      <td>12667</td>\n",
       "      <td>2034</td>\n",
       "      <td>516</td>\n",
       "      <td>131</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8964453</td>\n",
       "      <td>9</td>\n",
       "      <td>müzik çalar simülasyonu bil 211 - laboratuvar ...</td>\n",
       "      <td>105989</td>\n",
       "      <td>13683</td>\n",
       "      <td>2331</td>\n",
       "      <td>569</td>\n",
       "      <td>84</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2384475</td>\n",
       "      <td>2</td>\n",
       "      <td>2384475 recursion 1) collatz problemi. bu kısı...</td>\n",
       "      <td>129912</td>\n",
       "      <td>16263</td>\n",
       "      <td>2756</td>\n",
       "      <td>684</td>\n",
       "      <td>113</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4540765</td>\n",
       "      <td>10</td>\n",
       "      <td>4540765 recursion 1) collatz problemi. bu kısı...</td>\n",
       "      <td>117853</td>\n",
       "      <td>14921</td>\n",
       "      <td>2864</td>\n",
       "      <td>614</td>\n",
       "      <td>81</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6418019</td>\n",
       "      <td>4</td>\n",
       "      <td>6418019 recursion 1) collatz problemi. bu kısı...</td>\n",
       "      <td>130178</td>\n",
       "      <td>15554</td>\n",
       "      <td>2626</td>\n",
       "      <td>598</td>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4114373</td>\n",
       "      <td>2</td>\n",
       "      <td>4114373 recursion 1) collatz problemi. bu kısı...</td>\n",
       "      <td>146646</td>\n",
       "      <td>16779</td>\n",
       "      <td>3163</td>\n",
       "      <td>740</td>\n",
       "      <td>167</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5281670</td>\n",
       "      <td>5</td>\n",
       "      <td>5281670 müzik çalar simülasyonu bil 211 - labo...</td>\n",
       "      <td>100169</td>\n",
       "      <td>12914</td>\n",
       "      <td>1944</td>\n",
       "      <td>527</td>\n",
       "      <td>98</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1280995</td>\n",
       "      <td>1</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>158583</td>\n",
       "      <td>19100</td>\n",
       "      <td>3370</td>\n",
       "      <td>736</td>\n",
       "      <td>189</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2051651</td>\n",
       "      <td>8</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>44223</td>\n",
       "      <td>5549</td>\n",
       "      <td>635</td>\n",
       "      <td>250</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4779274</td>\n",
       "      <td>2</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>121960</td>\n",
       "      <td>15088</td>\n",
       "      <td>2291</td>\n",
       "      <td>630</td>\n",
       "      <td>117</td>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5187413</td>\n",
       "      <td>2</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>126605</td>\n",
       "      <td>16620</td>\n",
       "      <td>2949</td>\n",
       "      <td>715</td>\n",
       "      <td>183</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6914659</td>\n",
       "      <td>3</td>\n",
       "      <td>recursion 1) collatz problemi. bu kısımda amac...</td>\n",
       "      <td>117631</td>\n",
       "      <td>14812</td>\n",
       "      <td>2384</td>\n",
       "      <td>658</td>\n",
       "      <td>120</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  NaN_count                                            answers  \\\n",
       "0   4380745          1  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "1   8190737          4  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "2   8964453          9  müzik çalar simülasyonu bil 211 - laboratuvar ...   \n",
       "3   2384475          2  2384475 recursion 1) collatz problemi. bu kısı...   \n",
       "4   4540765         10  4540765 recursion 1) collatz problemi. bu kısı...   \n",
       "5   6418019          4  6418019 recursion 1) collatz problemi. bu kısı...   \n",
       "6   4114373          2  4114373 recursion 1) collatz problemi. bu kısı...   \n",
       "7   5281670          5  5281670 müzik çalar simülasyonu bil 211 - labo...   \n",
       "8   1280995          1  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "9   2051651          8  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "10  4779274          2  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "11  5187413          2  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "12  6914659          3  recursion 1) collatz problemi. bu kısımda amac...   \n",
       "\n",
       "    char_count  word_count  java_keyword_count  method_count  class_count  \\\n",
       "0       212997       24761                4249           990          157   \n",
       "1       103887       12667                2034           516          131   \n",
       "2       105989       13683                2331           569           84   \n",
       "3       129912       16263                2756           684          113   \n",
       "4       117853       14921                2864           614           81   \n",
       "5       130178       15554                2626           598          110   \n",
       "6       146646       16779                3163           740          167   \n",
       "7       100169       12914                1944           527           98   \n",
       "8       158583       19100                3370           736          189   \n",
       "9        44223        5549                 635           250           56   \n",
       "10      121960       15088                2291           630          117   \n",
       "11      126605       16620                2949           715          183   \n",
       "12      117631       14812                2384           658          120   \n",
       "\n",
       "    comment_count  \n",
       "0             378  \n",
       "1              79  \n",
       "2             104  \n",
       "3             139  \n",
       "4             140  \n",
       "5             109  \n",
       "6              60  \n",
       "7             228  \n",
       "8             157  \n",
       "9              22  \n",
       "10            181  \n",
       "11            119  \n",
       "12            266  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "33e6199d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  9.78it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create embeddings for test data\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=1000)  # adjust max_features if needed\n",
    "tfidf_embeddings = tfidf_vectorizer.fit_transform(test_data[\"answers\"])\n",
    "\n",
    "hand_features = ['char_count', 'word_count', 'java_keyword_count', \n",
    "                 'method_count', 'class_count', \n",
    "                 'NaN_count', 'comment_count']\n",
    "\n",
    "X_struct = test_data[hand_features].fillna(0).values \n",
    "\n",
    "Test_X_tfidf = hstack([tfidf_embeddings, X_struct])\n",
    "\n",
    "#---------\n",
    "\n",
    "bert_embeddings = get_bert_embedding(test_data[\"answers\"].tolist())\n",
    "\n",
    "bert_array = np.array(bert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct  # already dense\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Now safely combine them\n",
    "Test_X_combined_bert = np.hstack([bert_array, X_struct_scaled])\n",
    "\n",
    "#---------\n",
    "\n",
    "codebert_embeddings = get_codebert_embedding(test_data[\"answers\"].tolist())\n",
    "\n",
    "codebert_array = np.array(codebert_embeddings)\n",
    "\n",
    "if hasattr(X_struct, \"toarray\"):\n",
    "    X_struct_dense = X_struct.toarray()\n",
    "else:\n",
    "    X_struct_dense = X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "Test_X_combined_codebert = np.hstack([codebert_array, X_struct_scaled])\n",
    "\n",
    "#---------\n",
    "tokenized_texts = test_data[\"answers\"].apply(simple_preprocess).tolist()\n",
    "\n",
    "word2vec_array = np.array([average_vector(tokens, w2v_model, 100) for tokens in tokenized_texts])\n",
    "    \n",
    "X_struct_dense = X_struct.toarray() if hasattr(X_struct, \"toarray\") else X_struct\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_struct_scaled = scaler.fit_transform(X_struct_dense)\n",
    "\n",
    "# Combine\n",
    "Test_X_combined_w2v = np.hstack([word2vec_array, X_struct_scaled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "997af301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00,  8.97it/s]\n",
      "100%|██████████| 13/13 [00:03<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use same TF-IDF vectorizer trained on train_data\n",
    "Test_tfidf_embeddings = tfidf_vectorizer.transform(test_data[\"answers\"])\n",
    "Test_X_struct = test_data[hand_features].fillna(0).values\n",
    "\n",
    "# Only transform structured features (do not re-fit!)\n",
    "Test_X_struct_dense = Test_X_struct.toarray() if hasattr(Test_X_struct, \"toarray\") else Test_X_struct\n",
    "Test_X_struct_scaled = scaler.transform(Test_X_struct_dense)\n",
    "\n",
    "Test_X_tfidf = hstack([Test_tfidf_embeddings, Test_X_struct_dense])  # No need to scale for sparse\n",
    "\n",
    "# === BERT ===\n",
    "test_bert_embeddings = get_bert_embedding(test_data[\"answers\"].tolist())\n",
    "test_bert_array = np.array(test_bert_embeddings)\n",
    "Test_X_combined_bert = np.hstack([test_bert_array, Test_X_struct_scaled])\n",
    "\n",
    "# === CodeBERT ===\n",
    "test_codebert_embeddings = get_codebert_embedding(test_data[\"answers\"].tolist())\n",
    "test_codebert_array = np.array(test_codebert_embeddings)\n",
    "Test_X_combined_codebert = np.hstack([test_codebert_array, Test_X_struct_scaled])\n",
    "\n",
    "# === Word2Vec ===\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "test_tokenized = [tokenize(text) for text in test_data[\"answers\"]]  # replace with your tokenizer\n",
    "test_word2vec_array = np.array([average_vector(tokens, w2v_model, 100) for tokens in test_tokenized])\n",
    "Test_X_combined_w2v = np.hstack([test_word2vec_array, Test_X_struct_scaled])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ed60c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Embedding: Word2Vec ===\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 107 features, but StandardScaler is expecting 7 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m X  \u001b[38;5;66;03m# TF-IDF is already sparse and doesn't need scaling\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 18\u001b[0m     X_test \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Transform using the scaler fitted on training data\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Predict using the best model from previous grid search\u001b[39;00m\n\u001b[0;32m     21\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1062\u001b[0m, in \u001b[0;36mStandardScaler.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1059\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1061\u001b[0m copy \u001b[38;5;241m=\u001b[39m copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy\n\u001b[1;32m-> 1062\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n",
      "File \u001b[1;32mc:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\utils\\validation.py:2965\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2962\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m-> 2965\u001b[0m     \u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2967\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Talha\\miniconda3\\envs\\bitirme\\lib\\site-packages\\sklearn\\utils\\validation.py:2829\u001b[0m, in \u001b[0;36m_check_n_features\u001b[1;34m(estimator, X, reset)\u001b[0m\n\u001b[0;32m   2826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m-> 2829\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2830\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2831\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2832\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 107 features, but StandardScaler is expecting 7 features as input."
     ]
    }
   ],
   "source": [
    "test_embeddings = {\n",
    "    \"Word2Vec\": Test_X_combined_w2v,\n",
    "    \"TF-IDF\": Test_X_tfidf,\n",
    "    \"BERT\": Test_X_combined_bert,\n",
    "    \"CodeBERT\": Test_X_combined_codebert\n",
    "}\n",
    "\n",
    "# Now we will get our data ready to test the test set.\n",
    "test_data = test_data.fillna(0)  # Fill NaN values in test data\n",
    "\n",
    "for embed_name, X in test_embeddings.items():\n",
    "    print(f\"\\n=== Embedding: {embed_name} ===\")\n",
    "    \n",
    "    # Use the same scaler fitted on training data\n",
    "    if embed_name == \"TF-IDF\":\n",
    "        X_test = X  # TF-IDF is already sparse and doesn't need scaling\n",
    "    else:\n",
    "        X_test = scaler.transform(X)  # Transform using the scaler fitted on training data\n",
    "    \n",
    "    # Predict using the best model from previous grid search\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Save predictions to a DataFrame\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'id': test_data['id'],  # Assuming 'id' is the identifier in test_data\n",
    "        'FinalClass': y_pred\n",
    "    })\n",
    "    \n",
    "    predictions_df.to_csv(f\"predictions/predictions_{embed_name}.csv\", index=False)\n",
    "    print(f\"Predictions saved for {embed_name} embedding.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bitirme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
